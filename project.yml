title: "EDS-Pseudonymisation"
description: |
  This project aims at detecting identifying entities at AP-HP's Clinical Data Warehouse:

  | Label            | Description                                                   |
  | ---------------- | ------------------------------------------------------------- |
  | `ADRESSE`        | Street address, eg `33 boulevard de Picpus`                   |
  | `DATE`           | Any absolute date other than a birthdate                      |
  | `DATE_NAISSANCE` | Birthdate                                                     |
  | `HOPITAL`        | Hospital name, eg `Hôpital Rothschild`                        |
  | `IPP`            | Internal AP-HP identifier for patients, displayed as a number |
  | `MAIL`           | Email address                                                 |
  | `NDA`            | Internal AP-HP identifier for visits, displayed as a number   |
  | `NOM`            | Any last name (patients, doctors, third parties)              |
  | `PRENOM`         | Any first name (patients, doctors, etc)                       |
  | `SECU`           | Social security number                                        |
  | `TEL`            | Any phone number                                              |
  | `VILLE`          | Any city                                                      |
  | `ZIP`            | Any zip code                                                  |

  To run the full pipeline (download, split and format the dataset, train the pipeline and package it), simply run :
  ```shell
  spacy project run all
  ```

  If the pipeline detects that a command has already been run, it skips it unless its inputs have changed.

# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  name: "pseudonymisation"
  lang: "fr"
  version: "0.1.0"
  dataset: "data.jsonl"
  train: "train.jsonl"
  dev: "dev.jsonl"
  test: "test.jsonl"
  seed: 0
  fraction: 200
  gpu_id: 0

env:
  registry_token: GITLAB_REGISTRY_TOKEN

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories:
  ["assets", "corpus", "configs", "training", "scripts", "packages", "output"]

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
  all:
    - partition
    - convert
    - train
    - evaluate
  prepare-data:
    - partition
    - convert

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:

  - name: "partition"
    help: "Split data between train, dev and test data. The development set is taken from the train split."
    script:
      - "python scripts/partition.py --input-path assets/${vars.dataset} --output-train assets/${vars.train} --output-dev assets/${vars.dev} --output-test assets/${vars.test} --fraction ${vars.fraction} --seed ${vars.seed}"

    deps:
      - "assets/${vars.dataset}"
      - "scripts/partition.py"
    outputs:
      - "assets/${vars.train}"
      - "assets/${vars.dev}"
      - "assets/${vars.test}"

  - name: "convert"
    help: "Convert the data to spaCy's binary format"
    script:
      - "python scripts/convert.py --lang ${vars.lang} --input-path assets/${vars.train} --output-path corpus/train.spacy"
      - "python scripts/convert.py --lang ${vars.lang} --input-path assets/${vars.dev} --output-path corpus/dev.spacy"
      - "python scripts/convert.py --lang ${vars.lang} --input-path assets/${vars.test} --output-path corpus/test.spacy"
    deps:
      - "assets/${vars.train}"
      - "assets/${vars.dev}"
      - "assets/${vars.test}"
      - "scripts/convert.py"
    outputs:
      - "corpus/train.spacy"
      - "corpus/dev.spacy"
      - "corpus/test.spacy"

  - name: "create-config"
    help: "Create a new config with an NER pipeline component"
    script:
      - "python -m spacy init config --lang ${vars.lang} --pipeline ner configs/config.cfg --force --gpu"
    outputs:
      - "configs/config.cfg"

  - name: "train"
    help: "Train the NER model"
    script:
      - "python -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy --gpu-id ${vars.gpu_id}"
    deps:
      - "configs/config.cfg"
      - "corpus/train.spacy"
      - "corpus/dev.spacy"
    outputs:
      - "training/model-best"

  - name: "evaluate"
    help: "Evaluate the model and export metrics"
    script:
      - "python -m spacy evaluate training/model-best corpus/dev.spacy --output training/metrics.json --gpu-id ${vars.gpu_id}"
    deps:
      - "corpus/dev.spacy"
      - "training/model-best"
    outputs:
      - "training/metrics.json"

  - name: "package"
    help: "Package the trained model as a pip package"
    script:
      - "python -m spacy package training/model-best packages --name ${vars.name} --version ${vars.version} --force"
    deps:
      - "training/model-best"
    outputs_no_cache:
      - "packages/${vars.lang}_${vars.name}-${vars.version}/dist/${vars.lang}_${vars.name}-${vars.version}.tar.gz"

  - name: "visualize-model"
    help: "Visualize the model's output interactively using Streamlit"
    script:
      - 'streamlit run scripts/visualize_model.py training/model-best "Le patient M. Durand habite à Paris."'
    deps:
      - "scripts/visualize_model.py"
      - "training/model-best"
    no_skip: true
